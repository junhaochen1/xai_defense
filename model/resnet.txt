D:\Users\cjh\anaconda3\python.exe C:/Users/cjh/Desktop/bishe/miniimage/resnet_augmentation_train/resnet18_train.py
Found 28800 files belonging to 15 classes.
Found 1800 files belonging to 15 classes.
sample: (38, 64, 64, 3) (38,) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)
Model: "resnet18"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            
__________________________________________________________________________________________________
conv2d (Conv2D)                 (None, 62, 62, 64)   1792        input_1[0][0]                    
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, 62, 62, 64)   256         conv2d[0][0]                     
__________________________________________________________________________________________________
re_lu (ReLU)                    (None, 62, 62, 64)   0           batch_normalization[0][0]        
__________________________________________________________________________________________________
max_pooling2d (MaxPooling2D)    (None, 62, 62, 64)   0           re_lu[0][0]                      
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 62, 62, 64)   36928       max_pooling2d[0][0]              
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, 62, 62, 64)   256         conv2d_1[0][0]                   
__________________________________________________________________________________________________
activation (Activation)         (None, 62, 62, 64)   0           batch_normalization_1[0][0]      
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 62, 62, 64)   36928       activation[0][0]                 
__________________________________________________________________________________________________
batch_normalization_2 (BatchNor (None, 62, 62, 64)   256         conv2d_2[0][0]                   
__________________________________________________________________________________________________
add (Add)                       (None, 62, 62, 64)   0           batch_normalization_2[0][0]      
                                                                 max_pooling2d[0][0]              
__________________________________________________________________________________________________
activation_1 (Activation)       (None, 62, 62, 64)   0           add[0][0]                        
__________________________________________________________________________________________________
conv2d_3 (Conv2D)               (None, 62, 62, 64)   36928       activation_1[0][0]               
__________________________________________________________________________________________________
batch_normalization_3 (BatchNor (None, 62, 62, 64)   256         conv2d_3[0][0]                   
__________________________________________________________________________________________________
activation_2 (Activation)       (None, 62, 62, 64)   0           batch_normalization_3[0][0]      
__________________________________________________________________________________________________
conv2d_4 (Conv2D)               (None, 62, 62, 64)   36928       activation_2[0][0]               
__________________________________________________________________________________________________
batch_normalization_4 (BatchNor (None, 62, 62, 64)   256         conv2d_4[0][0]                   
__________________________________________________________________________________________________
add_1 (Add)                     (None, 62, 62, 64)   0           batch_normalization_4[0][0]      
                                                                 activation_1[0][0]               
__________________________________________________________________________________________________
activation_3 (Activation)       (None, 62, 62, 64)   0           add_1[0][0]                      
__________________________________________________________________________________________________
conv2d_5 (Conv2D)               (None, 31, 31, 128)  73856       activation_3[0][0]               
__________________________________________________________________________________________________
batch_normalization_5 (BatchNor (None, 31, 31, 128)  512         conv2d_5[0][0]                   
__________________________________________________________________________________________________
activation_4 (Activation)       (None, 31, 31, 128)  0           batch_normalization_5[0][0]      
__________________________________________________________________________________________________
conv2d_6 (Conv2D)               (None, 31, 31, 128)  147584      activation_4[0][0]               
__________________________________________________________________________________________________
batch_normalization_6 (BatchNor (None, 31, 31, 128)  512         conv2d_6[0][0]                   
__________________________________________________________________________________________________
conv2d_7 (Conv2D)               (None, 31, 31, 128)  8320        activation_3[0][0]               
__________________________________________________________________________________________________
add_2 (Add)                     (None, 31, 31, 128)  0           batch_normalization_6[0][0]      
                                                                 conv2d_7[0][0]                   
__________________________________________________________________________________________________
activation_5 (Activation)       (None, 31, 31, 128)  0           add_2[0][0]                      
__________________________________________________________________________________________________
conv2d_8 (Conv2D)               (None, 31, 31, 128)  147584      activation_5[0][0]               
__________________________________________________________________________________________________
batch_normalization_7 (BatchNor (None, 31, 31, 128)  512         conv2d_8[0][0]                   
__________________________________________________________________________________________________
activation_6 (Activation)       (None, 31, 31, 128)  0           batch_normalization_7[0][0]      
__________________________________________________________________________________________________
conv2d_9 (Conv2D)               (None, 31, 31, 128)  147584      activation_6[0][0]               
__________________________________________________________________________________________________
batch_normalization_8 (BatchNor (None, 31, 31, 128)  512         conv2d_9[0][0]                   
__________________________________________________________________________________________________
add_3 (Add)                     (None, 31, 31, 128)  0           batch_normalization_8[0][0]      
                                                                 activation_5[0][0]               
__________________________________________________________________________________________________
activation_7 (Activation)       (None, 31, 31, 128)  0           add_3[0][0]                      
__________________________________________________________________________________________________
conv2d_10 (Conv2D)              (None, 16, 16, 256)  295168      activation_7[0][0]               
__________________________________________________________________________________________________
batch_normalization_9 (BatchNor (None, 16, 16, 256)  1024        conv2d_10[0][0]                  
__________________________________________________________________________________________________
activation_8 (Activation)       (None, 16, 16, 256)  0           batch_normalization_9[0][0]      
__________________________________________________________________________________________________
conv2d_11 (Conv2D)              (None, 16, 16, 256)  590080      activation_8[0][0]               
__________________________________________________________________________________________________
batch_normalization_10 (BatchNo (None, 16, 16, 256)  1024        conv2d_11[0][0]                  
__________________________________________________________________________________________________
conv2d_12 (Conv2D)              (None, 16, 16, 256)  33024       activation_7[0][0]               
__________________________________________________________________________________________________
add_4 (Add)                     (None, 16, 16, 256)  0           batch_normalization_10[0][0]     
                                                                 conv2d_12[0][0]                  
__________________________________________________________________________________________________
activation_9 (Activation)       (None, 16, 16, 256)  0           add_4[0][0]                      
__________________________________________________________________________________________________
conv2d_13 (Conv2D)              (None, 16, 16, 256)  590080      activation_9[0][0]               
__________________________________________________________________________________________________
batch_normalization_11 (BatchNo (None, 16, 16, 256)  1024        conv2d_13[0][0]                  
__________________________________________________________________________________________________
activation_10 (Activation)      (None, 16, 16, 256)  0           batch_normalization_11[0][0]     
__________________________________________________________________________________________________
conv2d_14 (Conv2D)              (None, 16, 16, 256)  590080      activation_10[0][0]              
__________________________________________________________________________________________________
batch_normalization_12 (BatchNo (None, 16, 16, 256)  1024        conv2d_14[0][0]                  
__________________________________________________________________________________________________
add_5 (Add)                     (None, 16, 16, 256)  0           batch_normalization_12[0][0]     
                                                                 activation_9[0][0]               
__________________________________________________________________________________________________
activation_11 (Activation)      (None, 16, 16, 256)  0           add_5[0][0]                      
__________________________________________________________________________________________________
conv2d_15 (Conv2D)              (None, 8, 8, 512)    1180160     activation_11[0][0]              
__________________________________________________________________________________________________
batch_normalization_13 (BatchNo (None, 8, 8, 512)    2048        conv2d_15[0][0]                  
__________________________________________________________________________________________________
activation_12 (Activation)      (None, 8, 8, 512)    0           batch_normalization_13[0][0]     
__________________________________________________________________________________________________
conv2d_16 (Conv2D)              (None, 8, 8, 512)    2359808     activation_12[0][0]              
__________________________________________________________________________________________________
batch_normalization_14 (BatchNo (None, 8, 8, 512)    2048        conv2d_16[0][0]                  
__________________________________________________________________________________________________
conv2d_17 (Conv2D)              (None, 8, 8, 512)    131584      activation_11[0][0]              
__________________________________________________________________________________________________
add_6 (Add)                     (None, 8, 8, 512)    0           batch_normalization_14[0][0]     
                                                                 conv2d_17[0][0]                  
__________________________________________________________________________________________________
activation_13 (Activation)      (None, 8, 8, 512)    0           add_6[0][0]                      
__________________________________________________________________________________________________
conv2d_18 (Conv2D)              (None, 8, 8, 512)    2359808     activation_13[0][0]              
__________________________________________________________________________________________________
batch_normalization_15 (BatchNo (None, 8, 8, 512)    2048        conv2d_18[0][0]                  
__________________________________________________________________________________________________
activation_14 (Activation)      (None, 8, 8, 512)    0           batch_normalization_15[0][0]     
__________________________________________________________________________________________________
conv2d_19 (Conv2D)              (None, 8, 8, 512)    2359808     activation_14[0][0]              
__________________________________________________________________________________________________
batch_normalization_16 (BatchNo (None, 8, 8, 512)    2048        conv2d_19[0][0]                  
__________________________________________________________________________________________________
add_7 (Add)                     (None, 8, 8, 512)    0           batch_normalization_16[0][0]     
                                                                 activation_13[0][0]              
__________________________________________________________________________________________________
activation_15 (Activation)      (None, 8, 8, 512)    0           add_7[0][0]                      
__________________________________________________________________________________________________
global_average_pooling2d (Globa (None, 512)          0           activation_15[0][0]              
__________________________________________________________________________________________________
dense (Dense)                   (None, 128)          65664       global_average_pooling2d[0][0]   
__________________________________________________________________________________________________
activation_16 (Activation)      (None, 128)          0           dense[0][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 15)           1935        activation_16[0][0]              
==================================================================================================
Total params: 11,247,247
Trainable params: 11,239,439
Non-trainable params: 7,808
__________________________________________________________________________________________________
100%|██████████| 758/758 [09:31<00:00,  1.33it/s]
0 loss: 1.68966543674469
0 acc: 0.5483333333333333
100%|██████████| 758/758 [09:24<00:00,  1.34it/s]
1 loss: 1.3490400314331055
  0%|          | 0/758 [00:00<?, ?it/s]1 acc: 0.455
100%|██████████| 758/758 [09:24<00:00,  1.34it/s]
2 loss: 1.2942335605621338
2 acc: 0.5022222222222222
100%|██████████| 758/758 [09:23<00:00,  1.34it/s]
3 loss: 0.9655008316040039
3 acc: 0.6194444444444445
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
4 loss: 1.0651109218597412
4 acc: 0.5961111111111111
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
5 loss: 0.6313899755477905
5 acc: 0.6166666666666667
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
6 loss: 0.48834705352783203
6 acc: 0.6411111111111111
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
7 loss: 0.476116806268692
  0%|          | 0/758 [00:00<?, ?it/s]7 acc: 0.5944444444444444
100%|██████████| 758/758 [09:24<00:00,  1.34it/s]
8 loss: 0.6369534730911255
  0%|          | 0/758 [00:00<?, ?it/s]8 acc: 0.6361111111111111
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
9 loss: 0.5590220093727112
9 acc: 0.6016666666666667
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
10 loss: 0.46280795335769653
10 acc: 0.7061111111111111
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
11 loss: 0.5351675152778625
  0%|          | 0/758 [00:00<?, ?it/s]11 acc: 0.7038888888888889
100%|██████████| 758/758 [09:23<00:00,  1.34it/s]
12 loss: 0.41168928146362305
12 acc: 0.6955555555555556
100%|██████████| 758/758 [09:24<00:00,  1.34it/s]
13 loss: 0.43116530776023865
13 acc: 0.69
100%|██████████| 758/758 [09:23<00:00,  1.34it/s]
14 loss: 0.5383156538009644
14 acc: 0.6838888888888889
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
15 loss: 0.41394510865211487
15 acc: 0.6883333333333334
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
16 loss: 0.5345366597175598
16 acc: 0.7388888888888889
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
17 loss: 0.4519837200641632
  0%|          | 0/758 [00:00<?, ?it/s]17 acc: 0.6994444444444444
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
18 loss: 0.4566808044910431
18 acc: 0.7244444444444444
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
19 loss: 0.43437427282333374
  0%|          | 0/758 [00:00<?, ?it/s]19 acc: 0.6894444444444444
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
20 loss: 0.44294920563697815
  0%|          | 0/758 [00:00<?, ?it/s]20 acc: 0.7316666666666667
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
21 loss: 0.4689543843269348
21 acc: 0.715
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
22 loss: 0.5249832272529602
  0%|          | 0/758 [00:00<?, ?it/s]22 acc: 0.6916666666666667
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
23 loss: 0.4479553699493408
23 acc: 0.7038888888888889
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
24 loss: 0.39673587679862976
24 acc: 0.7083333333333334
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
25 loss: 0.39085930585861206
25 acc: 0.7016666666666667
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
26 loss: 0.040073294192552567
26 acc: 0.7888888888888889
100%|██████████| 758/758 [09:25<00:00,  1.34it/s]
27 loss: 0.03883058577775955
27 acc: 0.7872222222222223
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
28 loss: 0.04073808342218399
28 acc: 0.7927777777777778
100%|██████████| 758/758 [09:23<00:00,  1.34it/s]
29 loss: 0.03846244513988495
29 acc: 0.7933333333333333
100%|██████████| 758/758 [09:23<00:00,  1.34it/s]
30 loss: 0.03957470506429672
30 acc: 0.7922222222222223
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
31 loss: 0.044752780348062515
31 acc: 0.7916666666666666
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
32 loss: 0.03823788836598396
32 acc: 0.7966666666666666
100%|██████████| 758/758 [09:26<00:00,  1.34it/s]
33 loss: 0.038028765469789505
  0%|          | 0/758 [00:00<?, ?it/s]33 acc: 0.795
100%|██████████| 758/758 [09:23<00:00,  1.34it/s]
34 loss: 0.03847542405128479
34 acc: 0.7955555555555556
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
35 loss: 0.03772340342402458
  0%|          | 0/758 [00:00<?, ?it/s]35 acc: 0.7944444444444444
100%|██████████| 758/758 [09:23<00:00,  1.34it/s]
36 loss: 0.03818507120013237
36 acc: 0.7922222222222223
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
37 loss: 0.037258848547935486
  0%|          | 0/758 [00:00<?, ?it/s]37 acc: 0.7777777777777778
100%|██████████| 758/758 [09:24<00:00,  1.34it/s]
38 loss: 0.03703220561146736
38 acc: 0.7883333333333333
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
39 loss: 0.03693999722599983
39 acc: 0.7966666666666666
100%|██████████| 758/758 [09:24<00:00,  1.34it/s]
40 loss: 0.036572668701410294
40 acc: 0.7944444444444444
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
41 loss: 0.036436840891838074
41 acc: 0.7977777777777778
100%|██████████| 758/758 [09:23<00:00,  1.34it/s]
42 loss: 0.03610611706972122
42 acc: 0.7972222222222223
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
43 loss: 0.03577210754156113
43 acc: 0.7927777777777778
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
44 loss: 0.03524297475814819
44 acc: 0.7938888888888889
100%|██████████| 758/758 [09:24<00:00,  1.34it/s]
45 loss: 0.034936726093292236
45 acc: 0.7916666666666666
100%|██████████| 758/758 [09:24<00:00,  1.34it/s]
46 loss: 0.03483773022890091
46 acc: 0.7972222222222223
100%|██████████| 758/758 [09:29<00:00,  1.33it/s]
47 loss: 0.03430416062474251
47 acc: 0.7988888888888889
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
48 loss: 0.033956777304410934
48 acc: 0.7972222222222223
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
49 loss: 0.033629510551691055
49 acc: 0.8038888888888889
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
50 loss: 0.03350856155157089
50 acc: 0.7972222222222223
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
51 loss: 0.03308885544538498
51 acc: 0.7977777777777778
100%|██████████| 758/758 [09:24<00:00,  1.34it/s]
52 loss: 0.03259813040494919
  0%|          | 0/758 [00:00<?, ?it/s]52 acc: 0.7994444444444444
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
53 loss: 0.03232723847031593
53 acc: 0.7944444444444444
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
54 loss: 0.03275531902909279
54 acc: 0.7866666666666666
100%|██████████| 758/758 [09:22<00:00,  1.35it/s]
55 loss: 0.031899165362119675
55 acc: 0.7933333333333333
100%|██████████| 758/758 [09:21<00:00,  1.35it/s]
56 loss: 0.03189704567193985
  0%|          | 0/758 [00:00<?, ?it/s]56 acc: 0.7927777777777778
100%|██████████| 758/758 [09:21<00:00,  1.35it/s]
57 loss: 0.031899888068437576
57 acc: 0.7961111111111111
100%|██████████| 758/758 [09:23<00:00,  1.35it/s]
58 loss: 0.031194569543004036
58 acc: 0.7983333333333333
100%|██████████| 758/758 [09:25<00:00,  1.34it/s]
59 loss: 0.03088684193789959
59 acc: 0.7788888888888889

Process finished with exit code 0
